<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Transformer Circuits (Unofficial)</title>
    <link>https://transformer-circuits.pub/</link>
    <description>Unofficial RSS feed generated from the homepage of transformer-circuits.pub</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Tue, 03 Feb 2026 07:10:27 +0000</lastBuildDate>
    <item>
      <title>Superposition, Memorization, and Double Descent Henighan et al., 2023</title>
      <link>https://transformer-circuits.pub/2023/toy-double-descent/index.html</link>
      <description>We have little mechanistic understanding of how deep learning
                    models overfit to their training data, despite it being a
                    central problem. Here we extend our previous work on toy models
                    to shed light on how models generalize beyond their training
                    data.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/toy-double-descent/index.html</guid>
      <pubDate>Wed, 24 May 2023 23:00:47 +0000</pubDate>
    </item>
    <item>
      <title>Exercises</title>
      <link>https://transformer-circuits.pub/2021/exercises/index.html</link>
      <description>Some exercises we've developed to improve our understanding of how neural
                    networks implement algorithms at the parameter level.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2021/exercises/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:29:37 +0000</pubDate>
    </item>
    <item>
      <title>A Mathematical Framework for Transformer Circuits Elhage et al., 2021</title>
      <link>https://transformer-circuits.pub/2021/framework/index.html</link>
      <description>Our early mathematical framework for reverse engineering models,
                    demonstrated by reverse engineering small toy models. paper</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2021/framework/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:29:39 +0000</pubDate>
    </item>
    <item>
      <title>Garcon</title>
      <link>https://transformer-circuits.pub/2021/garcon/index.html</link>
      <description>A description of our tooling for doing interpretability on large models.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2021/garcon/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:29:40 +0000</pubDate>
    </item>
    <item>
      <title>Videos</title>
      <link>https://transformer-circuits.pub/2021/videos/index.html</link>
      <description>Very rough informal talks as we search for a way to reverse engineering
                    transformers.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2021/videos/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:29:40 +0000</pubDate>
    </item>
    <item>
      <title>In-Context Learning and Induction Heads Olsson et al., 2022</title>
      <link>https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</link>
      <description>An exploration of the hypothesis that induction heads are the primary
                    mechanism behind in-context learning. We also report the existence of a previously unknown phase
                    change in transformers language models. paper</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:30:03 +0000</pubDate>
    </item>
    <item>
      <title>Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases</title>
      <link>https://transformer-circuits.pub/2022/mech-interp-essay/index.html</link>
      <description>An informal note on intuitions related to mechanistic interpretability.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2022/mech-interp-essay/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:30:04 +0000</pubDate>
    </item>
    <item>
      <title>Softmax Linear Units</title>
      <link>https://transformer-circuits.pub/2022/solu/index.html</link>
      <description>An alternative activation function increases the fraction of neurons which
                    appear to correspond to human-understandable concepts.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2022/solu/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:30:05 +0000</pubDate>
    </item>
    <item>
      <title>Toy Models of Superposition Elhage et al., 2022</title>
      <link>https://transformer-circuits.pub/2022/toy_model/index.html</link>
      <description>Neural networks often seem to pack many unrelated concepts into a single
                    neuron - a puzzling phenomenon known as 'polysemanticity'. In our latest interpretability work,
                    we build toy models where the origins and dynamics of polysemanticity can be fully understood.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2022/toy_model/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:30:06 +0000</pubDate>
    </item>
    <item>
      <title>Interpretability Dreams</title>
      <link>https://transformer-circuits.pub/2023/interpretability-dreams/index.html</link>
      <description>Our present research aims to create a foundation for mechanistic
                    interpretability research. In doing so, it's important to keep sight of what we're trying to lay
                    the foundations for.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/interpretability-dreams/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:30:06 +0000</pubDate>
    </item>
    <item>
      <title>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Bricken et al., 2023</title>
      <link>https://transformer-circuits.pub/2023/monosemantic-features/index.html</link>
      <description>Using a sparse autoencoder, we extract a large number of interpretable features from a one-layer
                    transformer.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/monosemantic-features/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:30:07 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — May 2023</title>
      <link>https://transformer-circuits.pub/2023/may-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/may-update/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:30:07 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — July 2023</title>
      <link>https://transformer-circuits.pub/2023/july-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/july-update/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:30:07 +0000</pubDate>
    </item>
    <item>
      <title>Privileged Bases in the Transformer Residual Stream</title>
      <link>https://transformer-circuits.pub/2023/privileged-basis/index.html</link>
      <description>Our mathematical theories of the Transformer architecture suggest that
                    individual coordinates in the residual stream should have no special
                    significance, but recent work has shown that this observation is false in practice.
                    We investigate this phenomenon and provisionally conclude that the per-dimension normalizers in
                    the Adam optimizer are to blame for the effect.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/privileged-basis/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:44:36 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — January 2024</title>
      <link>https://transformer-circuits.pub/2024/jan-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/jan-update/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:44:38 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — February 2024</title>
      <link>https://transformer-circuits.pub/2024/feb-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/feb-update/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:44:38 +0000</pubDate>
    </item>
    <item>
      <title>Reflections on Qualitative Research</title>
      <link>https://transformer-circuits.pub/2024/qualitative-essay/index.html</link>
      <description>Some opinionated thoughts on why interpretability research may have
                    qualitative aspects be more central than we're used to in other fields.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/qualitative-essay/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:44:38 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — March 2024</title>
      <link>https://transformer-circuits.pub/2024/march-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/march-update/index.html</guid>
      <pubDate>Mon, 10 Jun 2024 05:44:38 +0000</pubDate>
    </item>
    <item>
      <title>Distributed Representations: Composition &amp; Superposition</title>
      <link>https://transformer-circuits.pub/2023/superposition-composition/index.html</link>
      <description>An informal note on how "distributed representations" might be understood
                    as two different, competing strategies — "composition" and "superposition" — with quite
                    different properties.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2023/superposition-composition/index.html</guid>
      <pubDate>Tue, 11 Jun 2024 20:27:04 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — April 2024</title>
      <link>https://transformer-circuits.pub/2024/april-update/index.html</link>
      <description>A collection of small updates from the Anthropic Interpretability Team.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/april-update/index.html</guid>
      <pubDate>Thu, 05 Sep 2024 23:58:57 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — June 2024</title>
      <link>https://transformer-circuits.pub/2024/june-update/index.html</link>
      <description>A collection of small updates: topk and gated SAE investigation.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/june-update/index.html</guid>
      <pubDate>Thu, 05 Sep 2024 23:59:16 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — July 2024</title>
      <link>https://transformer-circuits.pub/2024/july-update/index.html</link>
      <description>A collection of small updates: five hurdles, linear representations, dark matter, pivot tables,
                    feature sensitivity.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/july-update/index.html</guid>
      <pubDate>Fri, 06 Sep 2024 00:02:27 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — September 2024</title>
      <link>https://transformer-circuits.pub/2024/september-update/index.html</link>
      <description>A collection of small updates: investigating successor heads, oversampling data in SAEs.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/september-update/index.html</guid>
      <pubDate>Wed, 02 Oct 2024 17:36:43 +0000</pubDate>
    </item>
    <item>
      <title>Using Dictionary Learning Features as Classifiers</title>
      <link>https://transformer-circuits.pub/2024/features-as-classifiers/index.html</link>
      <description>A preliminary note comparing feature-based and raw-activation based harmfulness classifiers.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/features-as-classifiers/index.html</guid>
      <pubDate>Wed, 16 Oct 2024 16:38:07 +0000</pubDate>
    </item>
    <item>
      <title>Sparse Crosscoders for Cross-Layer Features and Model Diffing</title>
      <link>https://transformer-circuits.pub/2024/crosscoders/index.html</link>
      <description>A preliminary note on a way to get consistent features across layers, and even models.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/crosscoders/index.html</guid>
      <pubDate>Mon, 28 Oct 2024 19:58:49 +0000</pubDate>
    </item>
    <item>
      <title>Stage-Wise Model Diffing</title>
      <link>https://transformer-circuits.pub/2024/model-diffing/index.html</link>
      <description>A preliminary note on model diffing through dictionary fine-tuning.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/model-diffing/index.html</guid>
      <pubDate>Tue, 17 Dec 2024 05:08:06 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — August 2024</title>
      <link>https://transformer-circuits.pub/2024/august-update/index.html</link>
      <description>A collection of small updates: interpretability evals, reproducing self-explanation.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/august-update/index.html</guid>
      <pubDate>Mon, 27 Jan 2025 20:09:40 +0000</pubDate>
    </item>
    <item>
      <title>Insights on Crosscoder Model Diffing</title>
      <link>https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html</link>
      <description>A preliminary note on using crosscoders to diff models.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html</guid>
      <pubDate>Wed, 19 Feb 2025 21:26:51 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — January 2025</title>
      <link>https://transformer-circuits.pub/2025/january-update/index.html</link>
      <description>A collection of small updates: dictionary learning optimization techniques.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/january-update/index.html</guid>
      <pubDate>Wed, 19 Mar 2025 18:13:33 +0000</pubDate>
    </item>
    <item>
      <title>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Templeton et al., 2024</title>
      <link>https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html</link>
      <description>Using a sparse autoencoder, we extract a large number of interpretable features from Claude 3
                    Sonnet. Some appear to be safety-relevant.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html</guid>
      <pubDate>Tue, 25 Mar 2025 21:50:57 +0000</pubDate>
    </item>
    <item>
      <title>Progress on Attention</title>
      <link>https://transformer-circuits.pub/2025/attention-update/index.html</link>
      <description>An update on our progress studying attention.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/attention-update/index.html</guid>
      <pubDate>Tue, 29 Apr 2025 14:34:48 +0000</pubDate>
    </item>
    <item>
      <title>On the Biology of a Large Language Model Lindsey et al., 2025</title>
      <link>https://transformer-circuits.pub/2025/attribution-graphs/biology.html</link>
      <description>We investigate the internal mechanisms used by Claude 3.5 Haiku — Anthropic's lightweight
                    production model — in a variety of contexts.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/attribution-graphs/biology.html</guid>
      <pubDate>Wed, 07 May 2025 15:58:35 +0000</pubDate>
    </item>
    <item>
      <title>Circuit Tracing: Revealing Computational Graphs in Language Models Ameisen et al., 2025</title>
      <link>https://transformer-circuits.pub/2025/attribution-graphs/methods.html</link>
      <description>We describe an approach to tracing the "step-by-step" computation involved when a model responds
                    to a single prompt.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/attribution-graphs/methods.html</guid>
      <pubDate>Wed, 07 May 2025 15:58:36 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — April 2025</title>
      <link>https://transformer-circuits.pub/2025/april-update/index.html</link>
      <description>A collection of small updates: jailbreaks, dense features, and spinning up on interpretability.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/april-update/index.html</guid>
      <pubDate>Tue, 13 May 2025 19:51:58 +0000</pubDate>
    </item>
    <item>
      <title>Sparse mixtures of linear transforms</title>
      <link>https://transformer-circuits.pub/2025/bulk-update/index.html</link>
      <description>We investigate sparse mixture of linear transforms (MOLT), a new approach to transcoders.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/bulk-update/index.html</guid>
      <pubDate>Sat, 26 Jul 2025 05:07:12 +0000</pubDate>
    </item>
    <item>
      <title>A Toy Model of Interference Weights</title>
      <link>https://transformer-circuits.pub/2025/interference-weights/index.html</link>
      <description>Unpacking "interference weights" in some more depth.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/interference-weights/index.html</guid>
      <pubDate>Wed, 30 Jul 2025 19:05:47 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — July 2025</title>
      <link>https://transformer-circuits.pub/2025/july-update/index.html</link>
      <description>A collection of small updates: revisiting A Mathematical Framework and applications of
                    interpretability to biology.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/july-update/index.html</guid>
      <pubDate>Tue, 05 Aug 2025 20:43:58 +0000</pubDate>
    </item>
    <item>
      <title>A Toy Model of Mechanistic (Un)Faithfulness</title>
      <link>https://transformer-circuits.pub/2025/faithfulness-toy-model/index.html</link>
      <description>When transcoders go awry.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/faithfulness-toy-model/index.html</guid>
      <pubDate>Thu, 07 Aug 2025 20:27:18 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — August 2025</title>
      <link>https://transformer-circuits.pub/2025/august-update/index.html</link>
      <description>A small update: How does a persona modify the assistant’s response?</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/august-update/index.html</guid>
      <pubDate>Thu, 28 Aug 2025 21:02:20 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — September 2025</title>
      <link>https://transformer-circuits.pub/2025/september-update/index.html</link>
      <description>A small update on features and in-context learning.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/september-update/index.html</guid>
      <pubDate>Mon, 29 Sep 2025 21:17:32 +0000</pubDate>
    </item>
    <item>
      <title>When Models Manipulate Manifolds: The Geometry of a Counting Task Gurnee et al., 2025</title>
      <link>https://transformer-circuits.pub/2025/linebreaks/index.html</link>
      <description>We find geometric structure underlying the mechanisms of a fundamental language model behavior.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/linebreaks/index.html</guid>
      <pubDate>Tue, 21 Oct 2025 17:57:57 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — October 2025</title>
      <link>https://transformer-circuits.pub/2025/october-update/index.html</link>
      <description>Small updates on visual features and dictionary initialization.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/october-update/index.html</guid>
      <pubDate>Fri, 07 Nov 2025 17:15:33 +0000</pubDate>
    </item>
    <item>
      <title>Tracing Attention Computation Through Feature Interactions Kamath et al., 2025</title>
      <link>https://transformer-circuits.pub/2025/attention-qk/index.html</link>
      <description>We describe and apply a method to explain attention patterns in terms of
                    feature interactions, and integrate this information into attribution graphs.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/attention-qk/index.html</guid>
      <pubDate>Mon, 10 Nov 2025 07:25:05 +0000</pubDate>
    </item>
    <item>
      <title>Circuits Updates — November 2025</title>
      <link>https://transformer-circuits.pub/2025/november-update/index.html</link>
      <description>A short update on harm pressure.</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/november-update/index.html</guid>
      <pubDate>Fri, 05 Dec 2025 18:24:54 +0000</pubDate>
    </item>
    <item>
      <title>Emergent Introspective Awareness in Large Language Models Lindsey, 2025 We find evidence that language models can introspect on their internal states.</title>
      <link>https://transformer-circuits.pub/2025/introspection/index.html</link>
      <description>--&gt;</description>
      <guid isPermaLink="false">https://transformer-circuits.pub/2025/introspection/index.html</guid>
      <pubDate>Fri, 02 Jan 2026 08:15:55 +0000</pubDate>
    </item>
  </channel>
</rss>
